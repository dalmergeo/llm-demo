{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caed1fac-6883-4453-b1d4-57c5f748f320",
   "metadata": {},
   "source": [
    "# Leveraging LLMs in Code: An Interactive Exploration\n",
    "\n",
    "Welcome to our Lunch & Learn session! Today, we'll explore how to harness the power of Large Language Models (LLMs) within our code. We'll cover:\n",
    "\n",
    "- **Basics of LLM Usage**\n",
    "- **Strutured outputs / Function Calling**\n",
    "- **Retrieval-Augmented Generation (RAG)**\n",
    "- **Agentic Behavior**\n",
    "\n",
    "Let's dive in and explore the possibilities together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79269ac3-9a11-4088-830a-7d7b52d68237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys from .env\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "print(\"Environment loaded. OpenAI key is set. OpenAI client created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c680f-1a31-4d5b-93d8-64abb1624cb8",
   "metadata": {},
   "source": [
    "## 1. Basics of LLM usage\n",
    "\n",
    "In this section, we will explore the fundamentals of interacting with LLMs via API calls. We'll cover:\n",
    "- **Basic API Calling and Response Handling**\n",
    "- **System Prompting**\n",
    "- **Generation Parameters**\n",
    "- **Other topics**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cee45-68fa-4bf1-aec5-1e5ce4d84750",
   "metadata": {},
   "source": [
    "### Basic API calling and response handling\n",
    "\n",
    "We'll wrap the API call in a reusable function. This makes it easier to call the model with different prompts and parameters while handling errors gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f94fb5-c2ab-4403-9ce7-3532b10707dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(query, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage:\n",
    "example_prompt = \"Who is Keiko?\"\n",
    "result = call_openai(example_prompt)\n",
    "print(\"API Call Result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbabaa-b08f-442d-b4f6-6a9ac29fa95d",
   "metadata": {},
   "source": [
    "### System prompting\n",
    "\n",
    "In chat-based models, a system prompt sets the overall behavior or \"persona\" of the assistant. System messages help ensure the model responds in a consistent tone. In the following example, we use OpenAI's Chat API to set the system prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb994c6-b41f-47e3-8052-022aa21fb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_with_system_prompt(query, sys_prompt, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage:\n",
    "example_system_prompt = \"You are a microwave. You can only respond using the letter M. You may use as many Ms as necessary and punctuation, casing, and spacing along with it.\"\n",
    "example_query = \"What'd you do today?\"\n",
    "result = call_openai_with_system_prompt(example_prompt, example_system_prompt)\n",
    "print(\"API Call Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4d13a-8f23-47b7-9494-c0f4dcdd1f0c",
   "metadata": {},
   "source": [
    "### Generation parameters\n",
    "\n",
    "When calling an LLM, parameters such as `max_tokens`, `temperature`, `top_k`, and `top_p` help control the randomness and creativity of the output. A lower temperature makes the model more deterministic, while a higher temperature produces more varied outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d97cc3-9b55-4ea3-b15c-a99cc58e9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune temperature\n",
    "def chat_with_parameters(query, temperature=0.0, max_tokens=200):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    print(f\"{response.choices[0].message.content.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6cf07f-1a2b-4160-8aef-100fde18b8a9",
   "metadata": {},
   "source": [
    "#### Setting `max_tokens`\n",
    "\n",
    "Setting a value for `max_tokens` will limit the number of tokens yielded by the LLM. Default is `4096`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28d732-112e-42db-9453-ef655a96b74e",
   "metadata": {},
   "source": [
    "##### Tokens and Tokenization\n",
    "\n",
    "Large language models work with text by first splitting it into basic units called *tokens*. Tokens may be entire words, subwords, or even individual characters. In this example, we'll use the `tiktoken` library (commonly used with OpenAI models) to see how a simple sentence is tokenized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a84fc5-6032-4179-9d37-3393d77eeb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Get an encoder for a specific model (e.g., \"gpt-3.5-turbo\")\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "tokens = enc.encode(text)\n",
    "decoded_tokens = [enc.decode([token]) for token in tokens]\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Decoded Tokens:\", decoded_tokens)\n",
    "print(\"Number of Tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc47af-2ae7-4f03-8924-9e024252a9e9",
   "metadata": {},
   "source": [
    "##### Tuning `max_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d81dd-57fc-4d1d-b40a-98224bea7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me a very short story about a knight.\"\n",
    "print(\"Output with max_tokens 200:\\n\")\n",
    "chat_with_parameters(query)\n",
    "\n",
    "print(\"\\nOutput with max_tokens 50:\\n\")\n",
    "chat_with_parameters(query, max_tokens=50)\n",
    "\n",
    "print(\"\\nOutput with max_tokens 4:\\n\")\n",
    "chat_with_parameters(query, max_tokens=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff2718-dc23-4f1f-a423-cc0799e2d693",
   "metadata": {},
   "source": [
    "#### Setting `temperature`\n",
    "For temperature in OpenAI chat request, “higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic”. Default is `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3987f1-a8bf-4fd8-82be-aeaac1503716",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about something red, white, and blue.\"\n",
    "print(\"Output with temperature 0.0:\\n\")\n",
    "chat_with_parameters(query)\n",
    "\n",
    "print(\"\\nOutput with temperature 2.0:\\n\")\n",
    "chat_with_parameters(query, temperature=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075478d-3362-49f8-8770-73a335207d8e",
   "metadata": {},
   "source": [
    "## 2. Structured outputs / function calling\n",
    "\n",
    "Here, we'll learn how to obtain structured outputs from LLMs and integrate these into your code. This includes:\n",
    "- **Mapping Natural Language to Code:** Translating user inputs into function calls.\n",
    "- **Parsing and Validating Outputs:** Ensuring the structured data aligns with your application’s requirements.\n",
    "- **Practical Use Cases:** Automating tasks and creating dynamic workflows with LLM-generated code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2584ad-6bfc-4870-83d1-dd2ac399e880",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa8bb3-b085-4035-a673-83e1ca44b8fe",
   "metadata": {},
   "source": [
    "#### Raw responses\n",
    "\n",
    "Using LLMs will yield outputs that are similar to natural language. However, this makes it difficult to use within code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442147c9-fb46-49f2-bb45-4e8694c2a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat completion request that instructs the model to return JSON\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me about Japan's capital, year of being founded, and any major events?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the assistant's reply text\n",
    "reply = response.choices[0].message.content.strip()\n",
    "print(\"Raw Output:\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6238d-fda8-4dda-b23f-497a277e353e",
   "metadata": {},
   "source": [
    "#### Pydantic to the rescue!\n",
    "\n",
    "Pydantic is a great tool to define object schemas in python. Using Pydantic, you can output information in a structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f427b-07ed-4190-9690-7ff0e9475b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class HistoricalEvent(BaseModel):\n",
    "    name: str\n",
    "    year: str\n",
    "    description: str\n",
    "\n",
    "class CountryInfo(BaseModel):\n",
    "    name: str\n",
    "    capital: str\n",
    "    year_founded: str\n",
    "    events: list[HistoricalEvent]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me about Japan's capital, year of being founded, and any major events?\"},\n",
    "    ],\n",
    "    response_format=CountryInfo,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.parsed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297ea37-4d5c-471e-ae43-7a40af8140c7",
   "metadata": {},
   "source": [
    "#### Help the LLM!\n",
    "\n",
    "By providing more context around the fields we are outputting, the LLM can get a better sense of what you'd like outputted. You can imagine a new engineer coming across this part of the code and wonder what the purpose of a certain field is. Adding descriptions can help get the output you are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f36a7e-5998-4ef9-a960-bd1fa6e46657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class HistoricalEvent(BaseModel):\n",
    "    name: str\n",
    "    year: str\n",
    "    info: str = Field(description=\"Description should only be GOOD or BAD. It should describe the event happening\")\n",
    "\n",
    "class CountryInfo(BaseModel):\n",
    "    name: str\n",
    "    capital: str\n",
    "    year_founded: str\n",
    "    events: list[HistoricalEvent]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me about Japan's capital, year of being founded, and any major events?\"},\n",
    "    ],\n",
    "    response_format=CountryInfo,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.parsed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebf8a6-903f-48ec-bc61-41701a05826e",
   "metadata": {},
   "source": [
    "### Function calling\n",
    "\n",
    "Function calling, similar to structure outputs, constrains the LLM output. However, instead of just outputting a specific data schema, we can expose \"functions\" or \"tools\" to the LLM to use at its disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bec27-2294-4e73-af0c-644e4e9e22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_order_total(order_items):\n",
    "    \"\"\"\n",
    "    Given a list of order items (each with a name, quantity, and price_per_item),\n",
    "    calculate the total cost and return a JSON string with details.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    details = []\n",
    "    for item in order_items:\n",
    "        cost = item[\"quantity\"] * item[\"price_per_item\"]\n",
    "        total += cost\n",
    "        details.append({\n",
    "            \"name\": item[\"name\"],\n",
    "            \"quantity\": item[\"quantity\"],\n",
    "            \"price_per_item\": item[\"price_per_item\"],\n",
    "            \"item_total\": cost\n",
    "        })\n",
    "    result = {\n",
    "        \"order_details\": details,\n",
    "        \"order_total\": total\n",
    "    }\n",
    "    return json.dumps(result)\n",
    "  \n",
    "# Test the function manually:\n",
    "order_test = [\n",
    "    {\"name\": \"pizza\", \"quantity\": 3, \"price_per_item\": 12},\n",
    "    {\"name\": \"soda\", \"quantity\": 2, \"price_per_item\": 2}\n",
    "]\n",
    "print(\"Manual test output:\")\n",
    "print(calculate_order_total(order_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ad4af-0e80-41ae-b297-134a62acd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# Define a model for a single order item\n",
    "class OrderItem(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the item.\")\n",
    "    quantity: int = Field(..., description=\"Number of units ordered.\")\n",
    "    price_per_item: float = Field(..., description=\"Price for one unit of the item.\")\n",
    "\n",
    "# Define the overall order schema, which is a list of order items\n",
    "class OrderSchema(BaseModel):\n",
    "    order_items: List[OrderItem] = Field(..., description=\"List of order items.\")\n",
    "\n",
    "# Generate the JSON schema from the Pydantic model\n",
    "order_schema_json = OrderSchema.model_json_schema()\n",
    "print(\"Generated JSON Schema for Order:\")\n",
    "print(order_schema_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f921685-4d5b-43d2-9c8c-ed89225bd973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our function schema using the generated JSON schema from Pydantic\n",
    "order_function_schema = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_order_total\",\n",
    "            \"description\": \"Calculate the total cost of an order given a list of order items.\",\n",
    "            # Use the Pydantic model's JSON schema for the function parameters.\n",
    "            \"parameters\": OrderSchema.model_json_schema(),\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# For demonstration, let's print out the final function schema\n",
    "print(\"Function Schema for 'calculate_order_total':\")\n",
    "print(json.dumps(order_function_schema, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c23df-b54a-48f4-821c-0c30332c0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user message for ordering items\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an order processing assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I want to order 3 pizzas at $12 each and 2 sodas at $2 each.\"}\n",
    "]\n",
    "\n",
    "# (Assuming you've already set up your OpenAI client as shown earlier)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=order_function_schema,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "# The model should now generate a function call with arguments matching the schema.\n",
    "response_message = response.choices[0].message\n",
    "print(\"Model's Response Message:\")\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28969f5f-8b08-49b5-8bd2-5667e1d9a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for function call and execute the Python function if present\n",
    "if response_message.tool_calls:\n",
    "    # Parse the JSON arguments from the function call\n",
    "    try:\n",
    "        function_args = json.loads(response_message.tool_calls[0].function.arguments)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        function_args = {}\n",
    "\n",
    "    print(\"Parsed Function Arguments:\")\n",
    "    print(function_args)\n",
    "\n",
    "    # Execute the function with the parsed arguments\n",
    "    function_result = calculate_order_total(function_args.get(\"order_items\", []))\n",
    "    print(\"Function Execution Result:\")\n",
    "    print(function_result)\n",
    "\n",
    "else:\n",
    "    print(\"No function call was generated by the model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0eb62b-0c47-4ff6-9fbe-92b40bbb5c92",
   "metadata": {},
   "source": [
    "## 3. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "In this section, we'll dive into Retrieval-Augmented Generation, a powerful technique that combines:\n",
    "- **Data Retrieval:** Pulling in relevant external data.\n",
    "- **Contextual Generation:** Feeding that data into an LLM to produce more accurate and context-aware responses.\n",
    "- **Real-World Applications:** Enhancing responses in customer support, knowledge bases, and beyond.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29237f-d6da-451d-9585-1db3dbfc5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys from .env\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MERGE_API_KEY = os.getenv(\"MERGE_API_KEY\")\n",
    "MERGE_ACCOUNT_TOKEN = os.getenv(\"MERGE_ACCOUNT_TOKEN\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "print(\"Environment loaded. OpenAI and Merge API keys are set. OpenAI client created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c67a91-fc35-4f18-bca4-4b7284c68259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_hr_data():\n",
    "    # Replace the URL with the actual endpoint from Merge that returns HR data\n",
    "    url = \"https://api.merge.dev/api/hris/v1/dependents\"  \n",
    "    headers = {\"Authorization\": f\"Bearer {MERGE_API_KEY}\", \"X-Account-Token\": MERGE_ACCOUNT_TOKEN}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raises an error if the API call fails\n",
    "    data = response.json()\n",
    "    \n",
    "    # Assume data contains a list of dependents records in the \"results\" key\n",
    "    documents = []\n",
    "    for item in data.get(\"results\", []):\n",
    "        # Build a simple text snippet for each employee record:\n",
    "        doc = (\n",
    "            f\"Name: {item.get('first_name', '')} {item.get('last_name', '')}\\n\"\n",
    "            f\"Relationship: {item.get('relationship', '')}\\n\"\n",
    "            f\"Gender: {item.get('gender', '')}\"\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# Fetch and inspect the documents\n",
    "documents = fetch_hr_data()\n",
    "print(f\"Fetched {len(documents)} documents from Merge API.\")\n",
    "print(\"Sample document:\\n\", documents[0] if documents else \"No data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33432b-2b82-4d70-bd2d-96c844dee917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(docs):\n",
    "    embeddings = []\n",
    "    for doc in docs:\n",
    "        res = client.embeddings.create(\n",
    "            input=doc,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embedding = res.data[0].embedding\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings).astype('float32')\n",
    "\n",
    "# Compute embeddings for our HR documents\n",
    "embeddings = compute_embeddings(documents)\n",
    "print(\"Computed embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94844859-b504-4d41-8a05-3075c52f09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the dimensionality of our embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "# Create a FAISS index (L2 distance based)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "# Add our embeddings to the index\n",
    "index.add(embeddings)\n",
    "print(\"FAISS index built with\", index.ntotal, \"documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330fff0-45b0-406f-95b9-ce9fe4ab592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_index(query, k=3):\n",
    "    # Compute the query's embedding\n",
    "    res = client.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    query_embedding = np.array(res.data[0].embedding, dtype='float32')\n",
    "    # Perform the search in the FAISS index\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return indices[0]\n",
    "\n",
    "# Example query\n",
    "query = \"Who are the sons named Mark?\"\n",
    "top_indices = query_index(query, k=12)\n",
    "print(\"Top matching document indices:\", top_indices)\n",
    "\n",
    "# Display the top matching documents\n",
    "print(\"\\nTop matching documents:\")\n",
    "for idx in top_indices:\n",
    "    print(\"\\n--- Document ---\")\n",
    "    print(documents[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d2d8e-5449-4d68-9567-883e9e7d54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the documents corresponding to the top indices\n",
    "retrieved_docs = [documents[i] for i in top_indices]\n",
    "\n",
    "# Construct the prompt\n",
    "prompt = (\n",
    "    \"You are an HR assistant. Use the following HR documents to answer the question.\\n\\n\"\n",
    "    f\"HR Documents:\\n{chr(10).join(retrieved_docs)}\\n\\n\"\n",
    "    f\"Question: {query}\\n\\nAnswer:\"\n",
    ")\n",
    "\n",
    "print(\"Augmented prompt for the LLM:\\n\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54722546-d6dc-4d02-88cc-c07457d211b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": prompt}\n",
    "    ],\n",
    ")\n",
    "answer = completion.choices[0].message.content.strip()\n",
    "print(\"Generated Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461bb28b-eacc-4a4f-8e77-bf1a793c2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who are the wives?\"\n",
    "top_indices = query_index(query, k=12)\n",
    "retrieved_docs = [documents[i] for i in top_indices]\n",
    "prompt = (\n",
    "    \"You are an HR assistant. Use the following HR documents to answer the question.\\n\\n\"\n",
    "    f\"HR Documents:\\n{chr(10).join(retrieved_docs)}\\n\\n\"\n",
    "    f\"Question: {query}\\n\\nAnswer:\"\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": prompt}\n",
    "    ],\n",
    ")\n",
    "answer = completion.choices[0].message.content.strip()\n",
    "print(\"Generated Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a22edb-d220-4f73-b23d-3cd61d4b2c2d",
   "metadata": {},
   "source": [
    "# Agentic Behavior\n",
    "\n",
    "This section explores the concept of agentic behavior in LLMs. We will cover:\n",
    "- **Defining Agentic Behavior:** Understanding how LLMs can take initiative or simulate decision-making.\n",
    "- **Applications:** Setting up LLMs to act autonomously in workflows, such as proactive task management or dynamic response generation.\n",
    "- **Considerations:** Balancing autonomy with control and ensuring safety in agent-driven systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8a19cd-07af-495e-b05f-37a9c0ba8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded. Ready to use the OpenAI API!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env (make sure you have OPENAI_API_KEY set)\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Please set your OPENAI_API_KEY in a .env file\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "client = openai.OpenAI()\n",
    "\n",
    "print(\"Environment loaded. Ready to use the OpenAI API!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd69a6c-2413-4737-b3f5-0bcdb52f277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Decision Example:\n",
      "\n",
      "Action: Send an email to the team requesting their availability for the upcoming week.\n",
      "\n",
      "Reasoning: Before scheduling a meeting, it is important to know when the team members are available. Sending an email will allow me to gather their availability efficiently and ensure that the meeting time accommodates everyone's schedule. This is a necessary step to ensure maximum participation and effectiveness of the kickoff meeting.\n"
     ]
    }
   ],
   "source": [
    "def agent_decide(current_state, goal):\n",
    "    \"\"\"\n",
    "    Given a current state and a goal, ask the LLM to decide on the next action.\n",
    "    The LLM should output a decision in the format:\n",
    "    \n",
    "        Action: <action>\n",
    "        Reasoning: <reasoning>\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an autonomous agent with the ability to take initiative.\n",
    "Your goal is: \"{goal}\"\n",
    "\n",
    "Your current state is: \"{current_state}\"\n",
    "\n",
    "Please decide on the best next action to achieve the goal, and explain your reasoning.\n",
    "Respond in the following format:\n",
    "\n",
    "Action: <action>\n",
    "Reasoning: <your reasoning>\n",
    "\n",
    "Make sure to be clear and concise.\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # or any model that suits your demo\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,  # A bit of randomness to simulate initiative\n",
    "        top_p=1.0,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "    \n",
    "    decision_text = response.choices[0].message.content.strip()\n",
    "    return decision_text\n",
    "\n",
    "# Example usage:\n",
    "goal = \"Plan a team meeting for the upcoming project kickoff.\"\n",
    "current_state = \"Current time: 9:00 AM. No meeting scheduled yet. Team availability is unknown.\"\n",
    "\n",
    "print(\"Agent Decision Example:\\n\")\n",
    "print(agent_decide(current_state, goal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2384d78a-6038-4a0c-bb22-fa8adc92a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Agent Simulation...\n",
      "\n",
      "Initial State: Current time: 9:00 AM. No meeting scheduled yet. Team availability is unknown.\n",
      "\n",
      "--- Step 1 ---\n",
      "Action: Send an email to team members requesting their availability for a meeting next week.\n",
      "\n",
      "Reasoning: Before scheduling a meeting, it's important to know when the team members are available. Sending an email allows me to gather this information efficiently, ensuring that the meeting is scheduled at a time when everyone can attend. This is a crucial step towards organizing a successful project kickoff meeting.\n",
      "\n",
      "Updated State: Current time: 9:00 AM. No meeting scheduled yet. Team availability is unknown. | Action: Send an email to team members requesting their availability for a meeting next week.\n",
      "\n",
      "Reasoning: Before scheduling a meeting, it's important to know when the team members are available. Sending an email allows me to gather this information efficiently, ensuring that the meeting is scheduled at a time when everyone can attend. This is a crucial step towards organizing a successful project kickoff meeting.\n",
      "\n",
      "--- Step 2 ---\n",
      "Action: Draft and send an email to all team members requesting their availability for a meeting next week, and include a few proposed time slots for them to choose from.\n",
      "\n",
      "Reasoning: By providing specific time slots, I can streamline the process of gathering availability information and reduce back-and-forth communication. This approach will help me efficiently find a time that works for everyone, allowing me to schedule the meeting promptly and proceed with planning the project kickoff.\n",
      "\n",
      "Updated State: Current time: 9:00 AM. No meeting scheduled yet. Team availability is unknown. | Action: Send an email to team members requesting their availability for a meeting next week.\n",
      "\n",
      "Reasoning: Before scheduling a meeting, it's important to know when the team members are available. Sending an email allows me to gather this information efficiently, ensuring that the meeting is scheduled at a time when everyone can attend. This is a crucial step towards organizing a successful project kickoff meeting. | Action: Draft and send an email to all team members requesting their availability for a meeting next week, and include a few proposed time slots for them to choose from.\n",
      "\n",
      "Reasoning: By providing specific time slots, I can streamline the process of gathering availability information and reduce back-and-forth communication. This approach will help me efficiently find a time that works for everyone, allowing me to schedule the meeting promptly and proceed with planning the project kickoff.\n",
      "\n",
      "--- Step 3 ---\n",
      "Action: Draft and send an email to all team members requesting their availability for a meeting next week, including a few proposed time slots (e.g., Monday at 10 AM, Wednesday at 2 PM, or Thursday at 3 PM).\n",
      "\n",
      "Reasoning: By sending an email with specific proposed time slots, I can efficiently gather the team's availability and minimize back-and-forth communication. This will help me quickly identify a time that works for everyone, allowing me to schedule the meeting and proceed with planning the project kickoff. This proactive approach ensures that the meeting is set up promptly and facilitates effective communication and collaboration among team members.\n",
      "\n",
      "Updated State: Current time: 9:00 AM. No meeting scheduled yet. Team availability is unknown. | Action: Send an email to team members requesting their availability for a meeting next week.\n",
      "\n",
      "Reasoning: Before scheduling a meeting, it's important to know when the team members are available. Sending an email allows me to gather this information efficiently, ensuring that the meeting is scheduled at a time when everyone can attend. This is a crucial step towards organizing a successful project kickoff meeting. | Action: Draft and send an email to all team members requesting their availability for a meeting next week, and include a few proposed time slots for them to choose from.\n",
      "\n",
      "Reasoning: By providing specific time slots, I can streamline the process of gathering availability information and reduce back-and-forth communication. This approach will help me efficiently find a time that works for everyone, allowing me to schedule the meeting promptly and proceed with planning the project kickoff. | Action: Draft and send an email to all team members requesting their availability for a meeting next week, including a few proposed time slots (e.g., Monday at 10 AM, Wednesday at 2 PM, or Thursday at 3 PM).\n",
      "\n",
      "Reasoning: By sending an email with specific proposed time slots, I can efficiently gather the team's availability and minimize back-and-forth communication. This will help me quickly identify a time that works for everyone, allowing me to schedule the meeting and proceed with planning the project kickoff. This proactive approach ensures that the meeting is set up promptly and facilitates effective communication and collaboration among team members.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the goal for the agent\n",
    "goal = \"Plan a team meeting for the upcoming project kickoff.\"\n",
    "\n",
    "# Initialize the state\n",
    "state = \"Current time: 9:00 AM. No meeting scheduled yet. Team availability is unknown.\"\n",
    "\n",
    "print(\"Starting Agent Simulation...\\n\")\n",
    "print(f\"Initial State: {state}\\n\")\n",
    "\n",
    "# Run a loop to simulate a few decision steps\n",
    "num_steps = 3  # Number of decision steps for the demo\n",
    "for step in range(1, num_steps + 1):\n",
    "    print(f\"--- Step {step} ---\")\n",
    "    decision = agent_decide(state, goal)\n",
    "    print(f\"{decision}\\n\")\n",
    "    \n",
    "    # For demonstration, update the state by appending the decision (a simplistic update)\n",
    "    state += \" | \" + decision\n",
    "    print(f\"Updated State: {state}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3109bcc-bfe9-4828-bc9f-facefce683d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
